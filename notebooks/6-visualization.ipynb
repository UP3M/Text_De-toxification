{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##This code will visualize the result of finetuned toxic pharapraser model, you can put a toxic sentence then it will be give the translation from the model"
      ],
      "metadata": {
        "id": "z7F4B_JgIHpY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCcrFb50H83Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "from tqdm.auto import tqdm, trange\n",
        "import gc\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"\n",
        "    A helpful function to clean all cached batches.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "#Reading the inputs\n",
        "\n",
        "toxic_inputs = df_train_toxic[:10]\n",
        "#Loading the model. For the baseline we used t5_small_model\n",
        "\n",
        "base_model_name = 't5-small'\n",
        "model_name = 't5_base_train_10000'\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.to(device);\n",
        "# Paraphrasing preparation with small example\n",
        "\n",
        "def paraphrase(text, model, n=None, max_length='auto', temperature=0.0, beams=3):\n",
        "    texts = [text] if isinstance(text, str) else text\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
        "    if max_length == 'auto':\n",
        "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
        "    result = model.generate(\n",
        "        inputs,\n",
        "        num_return_sequences=n or 1,\n",
        "        do_sample=False,\n",
        "        temperature=temperature,\n",
        "        repetition_penalty=3.0,\n",
        "        max_length=max_length,\n",
        "        bad_words_ids=[[2]],  # unk\n",
        "        num_beams=beams,\n",
        "    )\n",
        "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
        "    if not n and isinstance(text, str):\n",
        "        return texts[0]\n",
        "    return texts\n",
        "print(paraphrase(['you are idiot'], model, temperature=50.0, beams=10))"
      ]
    }
  ]
}